{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bidimensional_protein_folding import ProteinEnv\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "Q_dict = collections.defaultdict(dict)\n",
    "\n",
    "print(len(Q_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_to_dict(state):\n",
    "    if (state[0], tuple(state[1])) not in Q_dict.keys():\n",
    "        Q_dict[state[0], tuple(state[1])] = {}\n",
    "        for action in range(4):\n",
    "            Q_dict[state[0], tuple(state[1])][action] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([])\n"
     ]
    }
   ],
   "source": [
    "print(Q_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.1\n",
    "decay_rate = 0.000001\n",
    "\n",
    "def epsilon_greedy(state, time):\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate*time)\n",
    "    #print('epsilon ', epsilon)\n",
    "    z = np.random.random()\n",
    "    #print('z ', z)\n",
    "        \n",
    "    if z > epsilon:\n",
    "        action = max(Q_dict[state[0], tuple(state[1])],key=\n",
    "                     Q_dict[state[0], tuple(state[1])].get)   #Exploitation: this gets the action corresponding to max q-value of current state\n",
    "    else:\n",
    "        action = np.random.choice(np.arange(0,4))    #Exploration: randomly choosing and action\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(state,time):\n",
    "    e = 0\n",
    "    if np.random.rand(1) < e:\n",
    "        action = np.random.choice(np.arange(0,4))    #Exploration: randomly choosing and action\n",
    "    else:\n",
    "        action = max(Q_dict[state[0], tuple(state[1])],key=\n",
    "                     Q_dict[state[0], tuple(state[1])].get)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining parameters for the experiment\n",
    "\n",
    "EPISODES = 10\n",
    "LR = 0.0001                   #learning rate\n",
    "GAMMA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ##################################\n",
      "        # episode  0 \n",
      "    ###################################\n"
     ]
    }
   ],
   "source": [
    "for episode in range(0,EPISODES):\n",
    "    if episode % 100000 == 0:\n",
    "        print('''\n",
    "    ##################################\n",
    "        # episode ''',episode,'''\n",
    "    ###################################''')\n",
    "    env = ProteinEnv()      #creating an instance of the class\n",
    "    #print('protein ', env.protein)\n",
    "    curr_state = env.state\n",
    "    add_to_dict(curr_state)\n",
    "    reward = None    \n",
    "    \n",
    "    while len(curr_state[1])<len(curr_state[0]):  \n",
    "        \n",
    "        #print('current state [1] in while loop', curr_state[1])\n",
    "        #print('size state[1]=',len(curr_state[1]),' size state[0]=',len(curr_state[0]))\n",
    "        \n",
    "        curr_action = epsilon_greedy(curr_state, episode)\n",
    "        \n",
    "        next_state, reward = env.step(curr_state, curr_action)\n",
    "        #print('reward ', reward)\n",
    "        \n",
    "        add_to_dict(next_state)\n",
    "\n",
    "        # UPDATE RULE\n",
    "        \n",
    "        max_next = max(Q_dict[next_state[0], tuple(next_state[1])],key=Q_dict[next_state[0], tuple(next_state[1])].get)   #this gets the action corresponding to max q-value of next state\n",
    "\n",
    "        \n",
    "         \n",
    "        #print('Q_dict before', Q_dict[curr_state[0], tuple(curr_state[1])][curr_action])\n",
    "        \n",
    "        #without learning rate\n",
    "        Q_dict[curr_state[0], tuple(curr_state[1])][curr_action] =\\\n",
    "        reward + GAMMA*(Q_dict[next_state[0], tuple(next_state[1])][max_next]) \n",
    "        \n",
    "        #with learning rate\n",
    "        #Q_dict[curr_state[0], tuple(curr_state[1])][curr_action] =\\\n",
    "        #(1-LR)*Q_dict[curr_state[0], tuple(curr_state[1])][curr_action]+ LR *(reward + (GAMMA*(Q_dict[next_state[0], tuple(next_state[1])][max_next])))  \n",
    "\n",
    "\n",
    "        #print('Q_dict after', Q_dict[curr_state[0], tuple(curr_state[1])][curr_action])\n",
    "        \n",
    "        curr_state = next_state\n",
    "        #print(next_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HHPHHHPHH\n",
      "HHPHHHPHH\n",
      "((0, 0), (0, 1), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (2, 1), (3, 1))\n",
      "((0, 0), (0, -1), (0, 0))\n",
      "[(0, 0), (0, 1), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (2, 1), (3, 1)]\n",
      "[(0, 0), (0, -1), (0, 0)]\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### len(Q_dict)\n",
    "next_state=('HHPHHHPHH', [(0, 0), (0, -1), (0, 0)] )\n",
    "#max_next = max(Q_dict[next_state[0], tuple(next_state[1])],key=Q_dict[next_state[0], tuple(next_state[1])].get)\n",
    "print(curr_state[0])\n",
    "print(next_state[0])\n",
    "\n",
    "print(tuple(curr_state[1]))\n",
    "print(tuple(next_state[1]))\n",
    "\n",
    "print(curr_state[1])\n",
    "print(next_state[1])\n",
    "\n",
    "print(Q_dict[next_state[0], tuple(next_state[1])].get(1))\n",
    "\n",
    "ma = max(Q_dict[curr_state[0], tuple(curr_state[1])],key=\n",
    "                       Q_dict[curr_state[0], tuple(curr_state[1])].get) \n",
    "ma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Q_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function which will take as input the protein and return the arrangement/policy\n",
    "\n",
    "def policy(protein):\n",
    "    curr_state = (protein, [(0, 0)])\n",
    "    \n",
    "    while len(curr_state[1])<len(curr_state[0]):    \n",
    "        best_action = max(Q_dict[curr_state[0], tuple(curr_state[1])],key=Q_dict[curr_state[0], tuple(curr_state[1])].get)\n",
    "        next_state = env.transition(curr_state, best_action)\n",
    "        print('Q_dict after', Q_dict[curr_state[0], tuple(curr_state[1])][curr_action])\n",
    "        curr_state = next_state\n",
    "        #print('Q_dict after', Q_dict[curr_state[0], tuple(curr_state[1])][curr_action])\n",
    "\n",
    "    \n",
    "    x_h_coords = []\n",
    "    y_h_coords = []\n",
    "    x_p_coords = []\n",
    "    y_p_coords = []\n",
    "    \n",
    "    x_inter_coords = []\n",
    "    y_inter_coords = []\n",
    "    \n",
    "\n",
    "    for ix in range(len(next_state[0])):\n",
    "        if next_state[0][ix] == 'H':\n",
    "            x_h_coords.append(next_state[1][ix][0])\n",
    "            y_h_coords.append(next_state[1][ix][1])\n",
    "        if next_state[0][ix] == 'P':\n",
    "            x_p_coords.append(next_state[1][ix][0])\n",
    "            y_p_coords.append(next_state[1][ix][1])\n",
    "        \n",
    "       #inter H-P\n",
    "    for ix in range(len(next_state[0])-1):\n",
    "        x_inter_coords.append(((next_state[1][ix][0])+(next_state[1][ix+1][0]))/2)\n",
    "        y_inter_coords.append(((next_state[1][ix][1])+(next_state[1][ix+1][1]))/2)\n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    plt.scatter(x_h_coords, y_h_coords, marker = 'o', color = 'black')\n",
    "    plt.scatter(x_p_coords, y_p_coords, marker = 'o', facecolors='none', edgecolors='black')\n",
    "    plt.scatter(x_inter_coords, y_inter_coords, marker = 'o', color = 'red')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(x_h_coords)\n",
    "    print(y_h_coords)\n",
    "    print(x_p_coords)\n",
    "    print(y_p_coords)\n",
    "    print(x_inter_coords)\n",
    "    print(y_inter_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy('HHPHHH')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
